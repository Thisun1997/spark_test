{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparkTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJIz2xSbsBvpeTLc8IBsVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thisun1997/spark_test/blob/main/sparkTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBSnBTawQmch"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbMAQoGcQ3AA"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YavPx4QiRnpC",
        "outputId": "31c3fe77-ef2f-4fe8-f165-de728557f360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(os.listdir('./sample_data'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anscombe.json', 'README.md', 'mnist_test.csv', 'california_housing_test.csv', 'california_housing_train.csv', 'mnist_train_small.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PwPf92bSADF"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsEVNxkMSz30"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O_p6Auo4DyK",
        "outputId": "9cff1a88-04c6-4e74-c96a-b1349123c6da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-28 16:01:42--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.0.74.51, 34.205.238.171, 34.193.208.150, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.0.74.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  17.9MB/s    in 0.7s    \n",
            "\n",
            "2020-10-28 16:01:43 (17.9 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzpin8SV4Gmn",
        "outputId": "d09e4ad9-82b5-4cb6-b854-6fc65176c387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"tunnels\":[{\"name\":\"command_line\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://45cc5721e071.ngrok.io\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":1,\"gauge\":0,\"rate1\":0.00003087011809866623,\"rate5\":0.00094710155304635,\"rate15\":0.0007304586585316237,\"p50\":30084441177,\"p90\":30084441177,\"p95\":30084441177,\"p99\":30084441177},\"http\":{\"count\":1,\"rate1\":0.00001872367309579094,\"rate5\":0.000856972923876307,\"rate15\":0.0007065113752682741,\"p50\":79932596,\"p90\":79932596,\"p95\":79932596,\"p99\":79932596}}},{\"name\":\"command_line (http)\",\"uri\":\"/api/tunnels/command_line%20%28http%29\",\"public_url\":\"http://45cc5721e071.ngrok.io\",\"proto\":\"http\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":34,\"gauge\":0,\"rate1\":0.015610128001601617,\"rate5\":0.024577436694581643,\"rate15\":0.021311124000104275,\"p50\":679000105,\"p90\":62111033589,\"p95\":100307398560.5,\"p99\":116016234608},\"http\":{\"count\":97,\"rate1\":0.02273604989223986,\"rate5\":0.06969914383662835,\"rate15\":0.060924869568717184,\"p50\":4448356,\"p90\":41037541.00000001,\"p95\":68489055.09999998,\"p99\":79225476}}}],\"uri\":\"/api/tunnels\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te2ivcHAUUm3"
      },
      "source": [
        "rdd = sc.textFile('/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/data.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2VfiI5eVQX6",
        "outputId": "a0f3d36c-8b6a-44c1-90ef-6bcb12af9279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1999\\t1999', '2000\\t2000', '2001\\t2001', '2010\\t2010', '2018\\t2018']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkXGJIy4VR-U"
      },
      "source": [
        "df = rdd.map(lambda x: x.split(\"\\t\")).toDF([\"TEST\",\"TEST_PART\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28obGt5CVyVT",
        "outputId": "e474afb5-1212-4ba4-c642-805153c50167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---------+\n",
            "|TEST|TEST_PART|\n",
            "+----+---------+\n",
            "|1999|     1999|\n",
            "|2000|     2000|\n",
            "|2001|     2001|\n",
            "|2010|     2010|\n",
            "|2018|     2018|\n",
            "+----+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgKiV5FnGQu6",
        "outputId": "57aa4568-aca0-4563-8e04-76d26785e599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- TEST: string (nullable = true)\n",
            " |-- TEST_PART: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2uSj2VON9NJ",
        "outputId": "f93ababc-e1b3-453a-d681-491ee7d52a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "from pyspark.sql.types import DateType\n",
        "df1 = df.withColumn(\"TEST_COL\", df['TEST'].cast('long')).drop('TEST').withColumn('TEST_PART_COL', df['TEST_PART'].cast('long')).drop('TEST_PART')\n",
        "df1.show()\n",
        "print(df1.schema)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+\n",
            "|TEST_COL|TEST_PART_COL|\n",
            "+--------+-------------+\n",
            "|    1999|         1999|\n",
            "|    2000|         2000|\n",
            "|    2001|         2001|\n",
            "|    2010|         2010|\n",
            "|    2018|         2018|\n",
            "+--------+-------------+\n",
            "\n",
            "StructType(List(StructField(TEST_COL,LongType,true),StructField(TEST_PART_COL,LongType,true)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfaODRd1Uyo7",
        "outputId": "e65be3b8-60e5-4abb-fc33-06aab70c1765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "df1.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- TEST_COL: long (nullable = true)\n",
            " |-- TEST_PART_COL: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU_CEVisVDOz"
      },
      "source": [
        "df1.write.format(\"parquet\").partitionBy(\"TEST_PART_COL\").option(\"path\", \"/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/parquet\").saveAsTable(\"TEST_TABLE\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YOicMBR6ZP6",
        "outputId": "b21ea1f3-7a2e-4349-e190-92153d7de596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TEST_TABLE WHERE TEST_PART_COL = TEST_COL AND (TEST_COL = 2001 OR TEST_COL = 1999)\").explain(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL = 'TEST_COL) && (('TEST_COL = 2001) || ('TEST_COL = 1999)))\n",
            "   +- 'UnresolvedRelation `TEST_TABLE`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL: bigint, TEST_PART_COL: bigint\n",
            "Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- Filter ((TEST_PART_COL#33L = TEST_COL#32L) && ((TEST_COL#32L = cast(2001 as bigint)) || (TEST_COL#32L = cast(1999 as bigint))))\n",
            "   +- SubqueryAlias `default`.`test_table`\n",
            "      +- Relation[TEST_COL#32L,TEST_PART_COL#33L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- Filter ((((((TEST_PART_COL#33L = 2001) || (TEST_PART_COL#33L = 1999)) && isnotnull(TEST_PART_COL#33L)) && isnotnull(TEST_COL#32L)) && (TEST_PART_COL#33L = TEST_COL#32L)) && ((TEST_COL#32L = 2001) || (TEST_COL#32L = 1999)))\n",
            "   +- Relation[TEST_COL#32L,TEST_PART_COL#33L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- *(1) Filter ((isnotnull(TEST_COL#32L) && (TEST_PART_COL#33L = TEST_COL#32L)) && ((TEST_COL#32L = 2001) || (TEST_COL#32L = 1999)))\n",
            "   +- *(1) FileScan parquet default.test_table[TEST_COL#32L,TEST_PART_COL#33L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 2, PartitionFilters: [((TEST_PART_COL#33L = 2001) || (TEST_PART_COL#33L = 1999)), isnotnull(TEST_PART_COL#33L)], PushedFilters: [IsNotNull(TEST_COL), Or(EqualTo(TEST_COL,2001),EqualTo(TEST_COL,1999))], ReadSchema: struct<TEST_COL:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbSJN5ue6M_A",
        "outputId": "ae23e205-2e3b-4e12-8584-5e3d33646db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TEST_TABLE WHERE TEST_PART_COL = TEST_COL AND (TEST_COL > 1999 AND TEST_COL <= 2001)\").explain(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL = 'TEST_COL) && (('TEST_COL > 1999) && ('TEST_COL <= 2001)))\n",
            "   +- 'UnresolvedRelation `TEST_TABLE`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL: bigint, TEST_PART_COL: bigint\n",
            "Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- Filter ((TEST_PART_COL#33L = TEST_COL#32L) && ((TEST_COL#32L > cast(1999 as bigint)) && (TEST_COL#32L <= cast(2001 as bigint))))\n",
            "   +- SubqueryAlias `default`.`test_table`\n",
            "      +- Relation[TEST_COL#32L,TEST_PART_COL#33L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- Filter ((((((isnotnull(TEST_PART_COL#33L) && isnotnull(TEST_COL#32L)) && (TEST_PART_COL#33L <= 2001)) && (TEST_PART_COL#33L > 1999)) && (TEST_PART_COL#33L = TEST_COL#32L)) && (TEST_COL#32L > 1999)) && (TEST_COL#32L <= 2001))\n",
            "   +- Relation[TEST_COL#32L,TEST_PART_COL#33L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- *(1) Filter (((isnotnull(TEST_COL#32L) && (TEST_PART_COL#33L = TEST_COL#32L)) && (TEST_COL#32L > 1999)) && (TEST_COL#32L <= 2001))\n",
            "   +- *(1) FileScan parquet default.test_table[TEST_COL#32L,TEST_PART_COL#33L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 2, PartitionFilters: [isnotnull(TEST_PART_COL#33L), (TEST_PART_COL#33L <= 2001), (TEST_PART_COL#33L > 1999)], PushedFilters: [IsNotNull(TEST_COL), GreaterThan(TEST_COL,1999), LessThanOrEqual(TEST_COL,2001)], ReadSchema: struct<TEST_COL:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OBw5J2z9L0I",
        "outputId": "4c5fdadd-9c26-40b7-be31-035cc52b1f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TEST_TABLE WHERE TEST_PART_COL = ABS(TEST_COL) AND (TEST_COL = 2001 OR TEST_COL = 1999)\").explain(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL = 'ABS('TEST_COL)) && (('TEST_COL = 2001) || ('TEST_COL = 1999)))\n",
            "   +- 'UnresolvedRelation `TEST_TABLE`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL: bigint, TEST_PART_COL: bigint\n",
            "Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- Filter ((TEST_PART_COL#33L = abs(TEST_COL#32L)) && ((TEST_COL#32L = cast(2001 as bigint)) || (TEST_COL#32L = cast(1999 as bigint))))\n",
            "   +- SubqueryAlias `default`.`test_table`\n",
            "      +- Relation[TEST_COL#32L,TEST_PART_COL#33L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- Filter (((isnotnull(TEST_PART_COL#33L) && isnotnull(TEST_COL#32L)) && (TEST_PART_COL#33L = abs(TEST_COL#32L))) && ((TEST_COL#32L = 2001) || (TEST_COL#32L = 1999)))\n",
            "   +- Relation[TEST_COL#32L,TEST_PART_COL#33L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- *(1) Filter ((isnotnull(TEST_COL#32L) && (TEST_PART_COL#33L = abs(TEST_COL#32L))) && ((TEST_COL#32L = 2001) || (TEST_COL#32L = 1999)))\n",
            "   +- *(1) FileScan parquet default.test_table[TEST_COL#32L,TEST_PART_COL#33L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 5, PartitionFilters: [isnotnull(TEST_PART_COL#33L)], PushedFilters: [IsNotNull(TEST_COL), Or(EqualTo(TEST_COL,2001),EqualTo(TEST_COL,1999))], ReadSchema: struct<TEST_COL:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OjwoSDn9MmK",
        "outputId": "e7bfe1bb-7027-4465-a2ae-94416af16981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TEST_TABLE WHERE TEST_PART_COL = ABS(TEST_COL) AND (TEST_COL > 1999 AND TEST_COL <= 2001)\").explain(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL = 'ABS('TEST_COL)) && (('TEST_COL > 1999) && ('TEST_COL <= 2001)))\n",
            "   +- 'UnresolvedRelation `TEST_TABLE`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL: bigint, TEST_PART_COL: bigint\n",
            "Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- Filter ((TEST_PART_COL#33L = abs(TEST_COL#32L)) && ((TEST_COL#32L > cast(1999 as bigint)) && (TEST_COL#32L <= cast(2001 as bigint))))\n",
            "   +- SubqueryAlias `default`.`test_table`\n",
            "      +- Relation[TEST_COL#32L,TEST_PART_COL#33L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- Filter ((((isnotnull(TEST_PART_COL#33L) && isnotnull(TEST_COL#32L)) && (TEST_PART_COL#33L = abs(TEST_COL#32L))) && (TEST_COL#32L > 1999)) && (TEST_COL#32L <= 2001))\n",
            "   +- Relation[TEST_COL#32L,TEST_PART_COL#33L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL#32L, TEST_PART_COL#33L]\n",
            "+- *(1) Filter (((isnotnull(TEST_COL#32L) && (TEST_PART_COL#33L = abs(TEST_COL#32L))) && (TEST_COL#32L > 1999)) && (TEST_COL#32L <= 2001))\n",
            "   +- *(1) FileScan parquet default.test_table[TEST_COL#32L,TEST_PART_COL#33L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 5, PartitionFilters: [isnotnull(TEST_PART_COL#33L)], PushedFilters: [IsNotNull(TEST_COL), GreaterThan(TEST_COL,1999), LessThanOrEqual(TEST_COL,2001)], ReadSchema: struct<TEST_COL:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yxLEFUt7YKq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}