{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparkTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNA93r08Zv5oFI7DKiq1VqI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thisun1997/spark_test/blob/main/sparkTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBSnBTawQmch"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbMAQoGcQ3AA"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YavPx4QiRnpC",
        "outputId": "43b7e16f-6bb6-415f-c303-1b896ff875b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(os.listdir('./sample_data'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anscombe.json', 'README.md', 'mnist_test.csv', 'california_housing_test.csv', 'california_housing_train.csv', 'mnist_train_small.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PwPf92bSADF"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsEVNxkMSz30"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te2ivcHAUUm3"
      },
      "source": [
        "rdd = sc.textFile('/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/data.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2VfiI5eVQX6",
        "outputId": "18779546-0d05-463d-e233-be4b175859ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1999\\t1999', '2000\\t2000', '2001\\t2001', '2010\\t2010', '2018\\t2018']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkXGJIy4VR-U"
      },
      "source": [
        "df = rdd.map(lambda x: x.split(\"\\t\")).toDF([\"TEST\",\"TEST_PART\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28obGt5CVyVT",
        "outputId": "5959a6cb-afc7-40ce-8b49-698622f1b4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---------+\n",
            "|TEST|TEST_PART|\n",
            "+----+---------+\n",
            "|1999|     1999|\n",
            "|2000|     2000|\n",
            "|2001|     2001|\n",
            "|2010|     2010|\n",
            "|2018|     2018|\n",
            "+----+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgKiV5FnGQu6",
        "outputId": "ff7b6f34-0dd4-446f-a125-4495aca1fe1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- TEST: string (nullable = true)\n",
            " |-- TEST_PART: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2uSj2VON9NJ",
        "outputId": "614c2a4d-7c64-49e3-8a7c-40ed20b2f653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "from pyspark.sql.types import DateType\n",
        "df1 = df.withColumn(\"TEST_COL\", df['TEST'].cast('long')).drop('TEST').withColumn('TEST_PART_COL', df['TEST_PART'].cast('long')).drop('TEST_PART')\n",
        "df1.show()\n",
        "print(df1.schema)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+\n",
            "|TEST_COL|TEST_PART_COL|\n",
            "+--------+-------------+\n",
            "|    1999|         1999|\n",
            "|    2000|         2000|\n",
            "|    2001|         2001|\n",
            "|    2010|         2010|\n",
            "|    2018|         2018|\n",
            "+--------+-------------+\n",
            "\n",
            "StructType(List(StructField(TEST_COL,LongType,true),StructField(TEST_PART_COL,LongType,true)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfaODRd1Uyo7",
        "outputId": "efd23947-b0f9-4820-9ed2-0bdc53009865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "df1.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- TEST_COL: long (nullable = true)\n",
            " |-- TEST_PART_COL: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU_CEVisVDOz"
      },
      "source": [
        "df1.write.format(\"parquet\").partitionBy(\"TEST_PART_COL\").option(\"path\", \"/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/parquet\").saveAsTable(\"TEST_TABLE\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YOicMBR6ZP6",
        "outputId": "209c2b25-b72f-4157-9a7f-3963dd5c8288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TEST_TABLE WHERE TEST_PART_COL = TEST_COL AND (TEST_COL = 2001 OR TEST_COL = 1999)\").explain(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL = 'TEST_COL) && (('TEST_COL = 2001) || ('TEST_COL = 1999)))\n",
            "   +- 'UnresolvedRelation `TEST_TABLE`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL: bigint, TEST_PART_COL: bigint\n",
            "Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- Filter ((TEST_PART_COL#54L = TEST_COL#53L) && ((TEST_COL#53L = cast(2001 as bigint)) || (TEST_COL#53L = cast(1999 as bigint))))\n",
            "   +- SubqueryAlias `default`.`test_table`\n",
            "      +- Relation[TEST_COL#53L,TEST_PART_COL#54L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- Filter ((((isnotnull(TEST_PART_COL#54L) && isnotnull(TEST_COL#53L)) && ((TEST_PART_COL#54L = 2001) || (TEST_PART_COL#54L = 1999))) && (TEST_PART_COL#54L = TEST_COL#53L)) && ((TEST_COL#53L = 2001) || (TEST_COL#53L = 1999)))\n",
            "   +- Relation[TEST_COL#53L,TEST_PART_COL#54L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- *(1) Filter ((isnotnull(TEST_COL#53L) && (TEST_PART_COL#54L = TEST_COL#53L)) && ((TEST_COL#53L = 2001) || (TEST_COL#53L = 1999)))\n",
            "   +- *(1) FileScan parquet default.test_table[TEST_COL#53L,TEST_PART_COL#54L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 2, PartitionFilters: [isnotnull(TEST_PART_COL#54L), ((TEST_PART_COL#54L = 2001) || (TEST_PART_COL#54L = 1999))], PushedFilters: [IsNotNull(TEST_COL), Or(EqualTo(TEST_COL,2001),EqualTo(TEST_COL,1999))], ReadSchema: struct<TEST_COL:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbSJN5ue6M_A",
        "outputId": "10fa975b-3264-4170-c806-73cfa39f2487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TEST_TABLE WHERE TEST_PART_COL = TEST_COL AND (TEST_COL > 1999 AND TEST_COL <= 2001)\").explain(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL = 'TEST_COL) && (('TEST_COL > 1999) && ('TEST_COL <= 2001)))\n",
            "   +- 'UnresolvedRelation `TEST_TABLE`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL: bigint, TEST_PART_COL: bigint\n",
            "Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- Filter ((TEST_PART_COL#54L = TEST_COL#53L) && ((TEST_COL#53L > cast(1999 as bigint)) && (TEST_COL#53L <= cast(2001 as bigint))))\n",
            "   +- SubqueryAlias `default`.`test_table`\n",
            "      +- Relation[TEST_COL#53L,TEST_PART_COL#54L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- Filter (((((((TEST_PART_COL#54L > 1999) && isnotnull(TEST_PART_COL#54L)) && (TEST_PART_COL#54L <= 2001)) && isnotnull(TEST_COL#53L)) && (TEST_PART_COL#54L = TEST_COL#53L)) && (TEST_COL#53L > 1999)) && (TEST_COL#53L <= 2001))\n",
            "   +- Relation[TEST_COL#53L,TEST_PART_COL#54L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- *(1) Filter (((isnotnull(TEST_COL#53L) && (TEST_PART_COL#54L = TEST_COL#53L)) && (TEST_COL#53L > 1999)) && (TEST_COL#53L <= 2001))\n",
            "   +- *(1) FileScan parquet default.test_table[TEST_COL#53L,TEST_PART_COL#54L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 2, PartitionFilters: [(TEST_PART_COL#54L > 1999), isnotnull(TEST_PART_COL#54L), (TEST_PART_COL#54L <= 2001)], PushedFilters: [IsNotNull(TEST_COL), GreaterThan(TEST_COL,1999), LessThanOrEqual(TEST_COL,2001)], ReadSchema: struct<TEST_COL:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OBw5J2z9L0I",
        "outputId": "f55eef1e-e130-4840-d10f-1293d056277d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TEST_TABLE WHERE TEST_PART_COL = ABS(TEST_COL) AND (TEST_COL = 2001 OR TEST_COL = 1999)\").explain(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL = 'ABS('TEST_COL)) && (('TEST_COL = 2001) || ('TEST_COL = 1999)))\n",
            "   +- 'UnresolvedRelation `TEST_TABLE`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL: bigint, TEST_PART_COL: bigint\n",
            "Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- Filter ((TEST_PART_COL#54L = abs(TEST_COL#53L)) && ((TEST_COL#53L = cast(2001 as bigint)) || (TEST_COL#53L = cast(1999 as bigint))))\n",
            "   +- SubqueryAlias `default`.`test_table`\n",
            "      +- Relation[TEST_COL#53L,TEST_PART_COL#54L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- Filter (((isnotnull(TEST_PART_COL#54L) && isnotnull(TEST_COL#53L)) && (TEST_PART_COL#54L = abs(TEST_COL#53L))) && ((TEST_COL#53L = 2001) || (TEST_COL#53L = 1999)))\n",
            "   +- Relation[TEST_COL#53L,TEST_PART_COL#54L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- *(1) Filter ((isnotnull(TEST_COL#53L) && (TEST_PART_COL#54L = abs(TEST_COL#53L))) && ((TEST_COL#53L = 2001) || (TEST_COL#53L = 1999)))\n",
            "   +- *(1) FileScan parquet default.test_table[TEST_COL#53L,TEST_PART_COL#54L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 5, PartitionFilters: [isnotnull(TEST_PART_COL#54L)], PushedFilters: [IsNotNull(TEST_COL), Or(EqualTo(TEST_COL,2001),EqualTo(TEST_COL,1999))], ReadSchema: struct<TEST_COL:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OjwoSDn9MmK",
        "outputId": "eba46548-c90f-49ac-9f4f-4e62624d938e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TEST_TABLE WHERE TEST_PART_COL = ABS(TEST_COL) AND (TEST_COL > 1999 AND TEST_COL <= 2001)\").explain(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL = 'ABS('TEST_COL)) && (('TEST_COL > 1999) && ('TEST_COL <= 2001)))\n",
            "   +- 'UnresolvedRelation `TEST_TABLE`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL: bigint, TEST_PART_COL: bigint\n",
            "Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- Filter ((TEST_PART_COL#54L = abs(TEST_COL#53L)) && ((TEST_COL#53L > cast(1999 as bigint)) && (TEST_COL#53L <= cast(2001 as bigint))))\n",
            "   +- SubqueryAlias `default`.`test_table`\n",
            "      +- Relation[TEST_COL#53L,TEST_PART_COL#54L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- Filter ((((isnotnull(TEST_PART_COL#54L) && isnotnull(TEST_COL#53L)) && (TEST_PART_COL#54L = abs(TEST_COL#53L))) && (TEST_COL#53L > 1999)) && (TEST_COL#53L <= 2001))\n",
            "   +- Relation[TEST_COL#53L,TEST_PART_COL#54L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL#53L, TEST_PART_COL#54L]\n",
            "+- *(1) Filter (((isnotnull(TEST_COL#53L) && (TEST_PART_COL#54L = abs(TEST_COL#53L))) && (TEST_COL#53L > 1999)) && (TEST_COL#53L <= 2001))\n",
            "   +- *(1) FileScan parquet default.test_table[TEST_COL#53L,TEST_PART_COL#54L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 5, PartitionFilters: [isnotnull(TEST_PART_COL#54L)], PushedFilters: [IsNotNull(TEST_COL), GreaterThan(TEST_COL,1999), LessThanOrEqual(TEST_COL,2001)], ReadSchema: struct<TEST_COL:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9-Cly6Fyhz"
      },
      "source": [
        "x = spark.sql(\"SELECT * FROM test_table WHERE TEST_PART_COL = TEST_COL AND (TEST_COL > 1999 AND TEST_COL <= 2001)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQPl4Kj_GKrz",
        "outputId": "cd69fe37-4c9f-424a-d339-d68515336da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "x.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+\n",
            "|TEST_COL|TEST_PART_COL|\n",
            "+--------+-------------+\n",
            "|    2001|         2001|\n",
            "|    2000|         2000|\n",
            "+--------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN8b4XvhZDRV"
      },
      "source": [
        "schema changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_Adfxg2Zb7p",
        "outputId": "fe34756c-ef06-4f4b-c221-fd14efb9cdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM TABLE_TEST WHERE TEST_PART_COL_LONG = TEST_COL_LONG AND (TEST_COL_LONG = 2001 OR TEST_COL_LONG = 1999)\").explain(True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter (('TEST_PART_COL_LONG = 'TEST_COL_LONG) && (('TEST_COL_LONG = 2001) || ('TEST_COL_LONG = 1999)))\n",
            "   +- 'UnresolvedRelation `TABLE_TEST`\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "TEST_COL_LONG: bigint, TEST_PART_COL_LONG: bigint\n",
            "Project [TEST_COL_LONG#522L, TEST_PART_COL_LONG#523L]\n",
            "+- Filter ((TEST_PART_COL_LONG#523L = TEST_COL_LONG#522L) && ((TEST_COL_LONG#522L = cast(2001 as bigint)) || (TEST_COL_LONG#522L = cast(1999 as bigint))))\n",
            "   +- SubqueryAlias `default`.`table_test`\n",
            "      +- Relation[TEST_COL_LONG#522L,TEST_PART_COL_LONG#523L] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [TEST_COL_LONG#522L, TEST_PART_COL_LONG#523L]\n",
            "+- Filter ((((((TEST_PART_COL_LONG#523L = 2001) || (TEST_PART_COL_LONG#523L = 1999)) && isnotnull(TEST_COL_LONG#522L)) && isnotnull(TEST_PART_COL_LONG#523L)) && (TEST_PART_COL_LONG#523L = TEST_COL_LONG#522L)) && ((TEST_COL_LONG#522L = 2001) || (TEST_COL_LONG#522L = 1999)))\n",
            "   +- Relation[TEST_COL_LONG#522L,TEST_PART_COL_LONG#523L] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [TEST_COL_LONG#522L, TEST_PART_COL_LONG#523L]\n",
            "+- *(1) Filter ((isnotnull(TEST_COL_LONG#522L) && (TEST_PART_COL_LONG#523L = TEST_COL_LONG#522L)) && ((TEST_COL_LONG#522L = 2001) || (TEST_COL_LONG#522L = 1999)))\n",
            "   +- *(1) FileScan parquet default.table_test[TEST_COL_LONG#522L,TEST_PART_COL_LONG#523L] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[file:/content/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/TestData/..., PartitionCount: 2, PartitionFilters: [((TEST_PART_COL_LONG#523L = 2001) || (TEST_PART_COL_LONG#523L = 1999)), isnotnull(TEST_PART_COL_..., PushedFilters: [IsNotNull(TEST_COL_LONG), Or(EqualTo(TEST_COL_LONG,2001),EqualTo(TEST_COL_LONG,1999))], ReadSchema: struct<TEST_COL_LONG:bigint>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MxdQh7sZr6H"
      },
      "source": [
        "# https://stackoverflow.com/questions/56994923/spark-subquery-scan-whole-partition"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}